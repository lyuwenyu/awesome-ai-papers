# Awesome AI Papers


## Github resource
Name | Info
---|---
[Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) | llm, mm-llm


## Computer Vision

name| Info
---|---
[Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/abs/2311.06242) | cv, foundation model

## Multimodal Large Language Model
name| Info
---|---
[MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/pdf/2401.13601.pdf) | mm-llm, survey
[A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549.pdf) | mm-llm, survey
[Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey](https://arxiv.org/pdf/2312.16602.pdf) | mm-llm, survey
[Visual Instruction Tuning](https://github.com/haotian-liu/LLaVA) | LLaVA, NeurIPS2023
[Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744.pdf) | LLaVA-1.5
[4M: Massively Multimodal Masked Modeling](https://arxiv.org/abs/2312.06647) | vision fundation model, apple
[OneLLM: One Framework to Align All Modalities with Language](https://arxiv.org/pdf/2312.03700.pdf) |  mllm, mmlab
[Pixel Aligned Language Models](https://jerryxu.net/PixelLLM/) |  pixel-llm, google
[Generative Multimodal Models are In-Context Learners](https://baaivision.github.io/emu2/) |  emu2, baai
[Data-Efficient Instruction Tuning for Alignment](https://arxiv.org/pdf/2312.15685.pdf) | Deita, sft
[CAPSFUSION: Rethinking Image-Text Data at Scale](https://arxiv.org/pdf/2310.20550.pdf) | data


